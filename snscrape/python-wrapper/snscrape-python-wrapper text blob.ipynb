{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Notebook for Scraping Twitter Using snscrape's Python Wrapper\n",
    "<br>Package Github: https://github.com/JustAnotherArchivist/snscrape\n",
    "<br>This notebook will be using the development version of snscrape\n",
    "\n",
    "Article Read-Along: https://medium.com/better-programming/how-to-scrape-tweets-with-snscrape-90124ed006af\n",
    "\n",
    "### Notebook Author: Martin Beck\n",
    "<b>Information current as of November, 28th 2020</b><br>\n",
    "\n",
    "This notebook contains materials for scraping tweets from Twitter using snscrape's Python Wrapper\n",
    "\n",
    "<b>Dependencies: </b> \n",
    "- Your <b>Python</b> version must be <b>3.8</b> or higher. The development version of snscrape will not work with Python 3.7 or lower. You can download the latest Python version [here](https://www.python.org/downloads/).\n",
    "- <b>Development version of snscrape</b>, uncomment the pip install line in the below cell to pip install in the notebook if you don't already have it.\n",
    "- <b>Pandas</b>, the dataframes allows easy manipulation and indexing of data, this is more of a preference but is what I follow in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 452, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3712/2702531126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\OneDrive - Microsoft Enterprise\\Documents\\GitHub\\TwitterScraper\\snscrape\\python-wrapper\\textblob.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# To read a CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./text-query-tweets-v2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 452, saw 5\n"
     ]
    }
   ],
   "source": [
    "# Run the pip install command below if you don't already have the library\n",
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Run the below command if you don't already have Pandas\n",
    "# !pip install pandas\n",
    "\n",
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by Username\n",
    "The code below will scrape for 100 tweets by a username then provide a CSV file with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sntwitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Amikom\\Documents\\GitHub\\TwitterScraper\\snscrape\\python-wrapper\\snscrape-python-wrapper text blob.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Amikom/Documents/GitHub/TwitterScraper/snscrape/python-wrapper/snscrape-python-wrapper%20text%20blob.ipynb#ch0000003?line=4'>5</a>\u001b[0m tweets_list1 \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Amikom/Documents/GitHub/TwitterScraper/snscrape/python-wrapper/snscrape-python-wrapper%20text%20blob.ipynb#ch0000003?line=6'>7</a>\u001b[0m \u001b[39m# Using TwitterSearchScraper to scrape data and append tweets to list\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Amikom/Documents/GitHub/TwitterScraper/snscrape/python-wrapper/snscrape-python-wrapper%20text%20blob.ipynb#ch0000003?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sntwitter\u001b[39m.\u001b[39mTwitterSearchScraper(\u001b[39m'\u001b[39m\u001b[39mfrom:jack\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget_items()):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Amikom/Documents/GitHub/TwitterScraper/snscrape/python-wrapper/snscrape-python-wrapper%20text%20blob.ipynb#ch0000003?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m>\u001b[39mmaxTweets:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Amikom/Documents/GitHub/TwitterScraper/snscrape/python-wrapper/snscrape-python-wrapper%20text%20blob.ipynb#ch0000003?line=9'>10</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sntwitter' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting variables to be used below\n",
    "maxTweets = 100\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list1 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:jack').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-27 21:25:36+00:00</td>\n",
       "      <td>1332435430801690624</td>\n",
       "      <td>@JesseDorogusker @Square ❤️</td>\n",
       "      <td>jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-18 19:49:02+00:00</td>\n",
       "      <td>1329149637006041088</td>\n",
       "      <td>@NeerajKA Welcome!</td>\n",
       "      <td>jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-18 18:59:50+00:00</td>\n",
       "      <td>1329137255026311168</td>\n",
       "      <td>Join @CashApp! #Bitcoin https://t.co/SbYANIZyix</td>\n",
       "      <td>jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-18 18:57:29+00:00</td>\n",
       "      <td>1329136665684705280</td>\n",
       "      <td>@kateconger @sarahintampa Nah</td>\n",
       "      <td>jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-18 18:54:05+00:00</td>\n",
       "      <td>1329135806192107521</td>\n",
       "      <td>@mmasnick Terrible idea! And terribly false.</td>\n",
       "      <td>jack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime             Tweet Id  \\\n",
       "0 2020-11-27 21:25:36+00:00  1332435430801690624   \n",
       "1 2020-11-18 19:49:02+00:00  1329149637006041088   \n",
       "2 2020-11-18 18:59:50+00:00  1329137255026311168   \n",
       "3 2020-11-18 18:57:29+00:00  1329136665684705280   \n",
       "4 2020-11-18 18:54:05+00:00  1329135806192107521   \n",
       "\n",
       "                                              Text Username  \n",
       "0                      @JesseDorogusker @Square ❤️     jack  \n",
       "1                               @NeerajKA Welcome!     jack  \n",
       "2  Join @CashApp! #Bitcoin https://t.co/SbYANIZyix     jack  \n",
       "3                    @kateconger @sarahintampa Nah     jack  \n",
       "4     @mmasnick Terrible idea! And terribly false.     jack  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df1.to_csv('user-tweets.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by Text Search\n",
    "The code below will scrape for 500 tweets between June 1st, 2020 and July 31st, 2020, by a text search then provide a CSV file with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used below\n",
    "maxTweets = 5000\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "all_prolarity = 0\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('xlhome since:2022-01-01 until:2022-01-30').get_items()):   \n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list2.append([tweet.content])\n",
    "\n",
    "    analysis = TextBlob(tweets_list2.content)\n",
    "    an = analysis.translate(from_lang='id', to='en')\n",
    "    print(an.sentiment)\n",
    "    all_prolarity += an.polarity\n",
    "if (all_prolarity/100 > 0):\n",
    "    print(all_prolarity/100)\n",
    "    print(\"\")\n",
    "    print('Positif')\n",
    "else:\n",
    "    print(all_prolarity/100)\n",
    "    print(\"\")\n",
    "    print('Negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-29 12:57:57+00:00</td>\n",
       "      <td>1487409682327404546</td>\n",
       "      <td>ini xlhome ngedown lagi kah loading mulu pas b...</td>\n",
       "      <td>kkareomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-28 16:36:51+00:00</td>\n",
       "      <td>1487102383427514369</td>\n",
       "      <td>@nawelshs i love xlhome</td>\n",
       "      <td>Atzer_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-28 11:03:22+00:00</td>\n",
       "      <td>1487018457803542528</td>\n",
       "      <td>@waguwagusuk ganti xlhome kakkkk, lancar jaya ...</td>\n",
       "      <td>yoshicat_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-28 06:41:23+00:00</td>\n",
       "      <td>1486952527882907648</td>\n",
       "      <td>@xlhomeid Duuh 4 hari internet mati tidak ada ...</td>\n",
       "      <td>FloraW44274560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-27 13:39:32+00:00</td>\n",
       "      <td>1486695368628731910</td>\n",
       "      <td>Sah! XL Axiata Akuisisi Link Net Senilai Rp 8,...</td>\n",
       "      <td>OniDewono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime             Tweet Id  \\\n",
       "0 2022-01-29 12:57:57+00:00  1487409682327404546   \n",
       "1 2022-01-28 16:36:51+00:00  1487102383427514369   \n",
       "2 2022-01-28 11:03:22+00:00  1487018457803542528   \n",
       "3 2022-01-28 06:41:23+00:00  1486952527882907648   \n",
       "4 2022-01-27 13:39:32+00:00  1486695368628731910   \n",
       "\n",
       "                                            Username            Text  \n",
       "0  ini xlhome ngedown lagi kah loading mulu pas b...        kkareomi  \n",
       "1                            @nawelshs i love xlhome          Atzer_  \n",
       "2  @waguwagusuk ganti xlhome kakkkk, lancar jaya ...       yoshicat_  \n",
       "3  @xlhomeid Duuh 4 hari internet mati tidak ada ...  FloraW44274560  \n",
       "4  Sah! XL Axiata Akuisisi Link Net Senilai Rp 8,...       OniDewono  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Username', 'Text'])\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df2.to_csv('text-query-tweets-v5.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
